---
title: "Yelp Negative Reviews ARM and Networking"
author: "Yunzhi Kong"
date: "10/18/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tokenizers)
library(viridis)
library(arules)
library(TSP)
library(data.table)
library(devtools)
library(purrr)
library(tidyr)
library(arulesViz)
```

Since the original data is too large, recall that in the Data Cleaning section, I randomly select 1000 rows from each year and created a new .csv file called "randomyelpbyyear.csv" to stored them. In this section I randomly pick 1000 negative reviews from this data set, to be more specific, 500 of them with 1 star and 500 of them with 2 stars, since I defined negative review as star equal to 1 or 2 in the previous section.

```{r}
Yelp<-read.csv("/Users/nikkkikong/randomyelpbyyear.csv",header = TRUE)
Yelp$stars<-as.factor(Yelp$stars)
# remove rows with reviews are extremely long
Yelp$num_word <- sapply(strsplit(x = Yelp$text, split = ' '), length)
Yelp<-Yelp%>%filter(num_word<100)
yelp_1<-Yelp%>%filter(Yelp$stars == "1") %>% sample_n(., 400) 
yelp_2<-Yelp%>%filter(Yelp$stars == "2") %>% sample_n(., 400) 
Yelp_neg<-rbind(yelp_1,yelp_2)
rm(Yelp)
rm(yelp_1)
rm(yelp_2)
#### Store this data set into csv file for future use
write.csv(Yelp_neg,"/Users/nikkkikong/Desktop/ANLY 501/project data/randomnegyelp.csv", row.names = FALSE)
```

### transfer the review text into transaction data
##### here I remove stopwords, punctuation, numbers, make word in lowercase. Every word is in a single column.

```{r}
Yelp_neg<-read.csv("/Users/nikkkikong/Desktop/ANLY 501/project data/randomnegyelp.csv",header = TRUE)
# Delete columns, keep the text data and "Label" only
Yelp_neg<-Yelp_neg[,-c(1,2,4,5,6)]
Yelp_neg<-as.data.frame(Yelp_neg)

## Start the file
TransactionreviewFile = "negreviewResult.csv"
# Trans <- file(TransactionreviewFile)
# ## Tokenize to words 
# ##
# Tokens<-tokenizers::tokenize_words(
#   Yelp_neg$Yelp_neg[1],stopwords = stopwords::stopwords("en"), 
#   lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,
#   simplify = TRUE)
# 
# ## Write tokens
# cat(unlist(Tokens), "\n", file=Trans, sep=",")
# close(Trans)

## Append remaining lists of tokens into file
## Recall - a list of tokens is the set of words from a Tweet
Trans <- file(TransactionreviewFile, open = "a")
for(i in 1:nrow(Yelp_neg)){
  Tokens<-tokenize_words(Yelp_neg$Yelp_neg[i],stopwords = stopwords::stopwords("en"), 
                         lowercase = TRUE,  strip_punct = TRUE, simplify = TRUE)
  cat(unlist(Tokens), "\n", file=Trans, sep=",")
}
close(Trans)
### review the cleaned transaction dataset
NegDF <- read.csv(TransactionreviewFile, 
                    header = FALSE, sep = ",")
head(NegDF)
## Convert all columns to char 
NegDF<-NegDF %>%
  mutate_all(as.character)
```

# More cleaning, remove digit and remove words with string length shorter than 4 and longer than 9.

```{r}
MyDF<-NULL
MyDF2<-NULL
for (i in 1:ncol(NegDF)){
  MyList=c() 
  MyList2=c() # each list is a column of logicals ...
  MyList=c(MyList,grepl("[[:digit:]]", NegDF[[i]]))
  MyDF<-cbind(MyDF,MyList)  ## create a logical DF
  MyList2=c(MyList2,(nchar(NegDF[[i]])<4 | nchar(NegDF[[i]])>9))
  MyDF2<-cbind(MyDF2,MyList2) 
  ## TRUE is when a cell has a word that contains digits
}
## For all TRUE, replace with blank
NegDF[MyDF] <- ""
NegDF[MyDF2] <- ""
(head(NegDF,10))
```


```{r}
# Now we save the dataframe using the write table command 
write.table(NegDF, file = "UpdatedTNegFile.csv", col.names = FALSE, 
            row.names = FALSE, sep = ",")
ReviewTrans <- read.transactions("UpdatedTNegFile.csv", sep =",", 
                                format("basket"),  rm.duplicates = TRUE)
```

### Create the Rules  - Relationships 

```{r}
ReviewTrans_rules = arules::apriori(ReviewTrans, 
        parameter = list(support=.01, conf=0.01, minlen=2))
inspect(ReviewTrans_rules)

##  SOrt by Conf
SortedRules_conf <- sort(ReviewTrans_rules, by="confidence", decreasing=TRUE)
inspect(SortedRules_conf[1:15])
## Sort by Sup
SortedRules_sup <- sort(ReviewTrans_rules, by="support", decreasing=TRUE)
inspect(SortedRules_sup[1:15])
## Sort by Lift
SortedRules_lift <- sort(ReviewTrans_rules, by="lift", decreasing=TRUE)
inspect(SortedRules_lift[1:15])
```

## Visulization

### Top 20 Frequent words

```{r}
## Plot of which items are most frequent
itemFrequencyPlot(ReviewTrans, topN=20, type="absolute",col = rainbow(7),xlab = "Top 20 frequent words", ylab = "Count")
```


#### Targeting Food rules LHS

```{r}

FoodRule <- apriori(ReviewTrans,parameter = list(supp=.01, conf=.01, minlen=2),
                     appearance = list(default="rhs", lhs="food"),
                     control=list(verbose=FALSE))
FoodRule_C <- sort(FoodRule, decreasing=TRUE, by="confidence")
FoodRule_S <- sort(FoodRule, decreasing=TRUE, by="support")
FoodRule_L <- sort(FoodRule, decreasing=TRUE, by="lift")


inspect(FoodRule_C[1:15])
inspect(FoodRule_S[1:15])
inspect(FoodRule_L[1:15])
```

```{r}

FoodsRule <- apriori(ReviewTrans,parameter = list(supp=.01, conf=.01, minlen=2),
                     appearance = list(default="rhs", lhs="sick"),
                     control=list(verbose=FALSE))
FoodRule_C <- sort(FoodRule, decreasing=TRUE, by="confidence")
FoodRule_S <- sort(FoodRule, decreasing=TRUE, by="support")
FoodRule_L <- sort(FoodRule, decreasing=TRUE, by="lift")


inspect(FoodsRule)
inspect(FoodRule_S[1:15])
inspect(FoodRule_L[1:15])
```

```{r}
plot(FoodRule_C[1:15], method="graph", 
     by = 'confidence',control=list(type="itemsets"), 
     main = 'Food Rules by confidence', shading = 'confidence')

plot(FoodRule_S[1:15], method="graph", 
     by = 'support',control=list(type="itemsets"), 
     main = 'Food Rules by support', shading = 'confidence')

plot(FoodRule_L[1:15], method="graph", 
     by = 'lift',control=list(type="itemsets"), 
     main = 'Food Rules by lift', shading = 'lift')

```



### Top 15 three measures visulizations

```{r}
library(igraph)
subrules <- head(sort(ReviewTrans_rules, by="support", decreasing=TRUE),15)
plot(subrules,main="Scatter plot for Top 15 Support")
plot(subrules, method="graph", engine="interactive",shading = "support")
inspect(subrules)

subrules <- head(sort(ReviewTrans_rules, by="confidence", decreasing=TRUE),15)
plot(subrules,main="Scatter plot for Top 15 Confidence")
plot(subrules, method="graph", engine="interactive",shading = "confidence")


subrules <- head(sort(ReviewTrans_rules, by="lift", decreasing=TRUE),15)
plot(subrules,main="Scatter plot for Top 15 Lift")
plot(subrules, method="graph", engine="interactive",shading = "lift")
```




## NetworkD3

```{r}
Rules_DF2<-DATAFRAME(ReviewTrans_rules, separate = TRUE)
(head(Rules_DF2))
str(Rules_DF2)
## Convert to char
Rules_DF2$LHS<-as.character(Rules_DF2$LHS)
Rules_DF2$RHS<-as.character(Rules_DF2$RHS)

## Remove all {}
Rules_DF2[] <- lapply(Rules_DF2, gsub, pattern='[{]', replacement='')
Rules_DF2[] <- lapply(Rules_DF2, gsub, pattern='[}]', replacement='')

head(Rules_DF2)


```

```{r}

## Remove the sup, conf, and count
## USING LIFT
Rules_L<-Rules_DF2[c(1,2,6)]
names(Rules_L) <- c("SourceName", "TargetName", "Weight")
head(Rules_L,30)

## USING SUP
Rules_S<-Rules_DF2[c(1,2,3)]
names(Rules_S) <- c("SourceName", "TargetName", "Weight")
head(Rules_S,30)

## USING CONF
Rules_C<-Rules_DF2[c(1,2,4)]
names(Rules_C) <- c("SourceName", "TargetName", "Weight")
head(Rules_C,30)

## CHoose and set
#Rules_Sup<-Rules_C
Rules_Sup<-Rules_L
#Rules_Sup<-Rules_S

```

```{r}
edgeList<-Rules_Sup
MyGraph <- igraph::simplify(igraph::graph.data.frame(edgeList, directed=TRUE))

nodeList <- data.frame(ID = c(0:(igraph::vcount(MyGraph) - 1)), 
                       # because networkD3 library requires IDs to start at 0
                       nName = igraph::V(MyGraph)$name)
## Node Degree
(nodeList <- cbind(nodeList, nodeDegree=igraph::degree(MyGraph, 
                    v = igraph::V(MyGraph), mode = "all")))

## Betweenness
BetweenNess <- igraph::betweenness(MyGraph, 
      v = igraph::V(MyGraph), 
      directed = TRUE) 

(nodeList <- cbind(nodeList, nodeBetweenness=BetweenNess))

## This can change the BetweenNess value if needed
BetweenNess<-BetweenNess/100


```

```{r}
getNodeID <- function(x){
  which(x == igraph::V(MyGraph)$name) - 1  #IDs start at 0
}
#(getNodeID("1")) 

edgeList <- plyr::ddply(
  Rules_Sup, .variables = c("SourceName", "TargetName" , "Weight"), 
  function (x) data.frame(SourceID = getNodeID(x$SourceName), 
                          TargetID = getNodeID(x$TargetName)))

head(edgeList)
nrow(edgeList)
```

```{r}
########################################################################
##############  Dice Sim ################################################
###########################################################################
#Calculate Dice similarities between all pairs of nodes
#The Dice similarity coefficient of two vertices is twice 
#the number of common neighbors divided by the sum of the degrees 
#of the vertices. Method dice calculates the pairwise Dice similarities 
#for some (or all) of the vertices. 
DiceSim <- igraph::similarity.dice(MyGraph, vids = igraph::V(MyGraph), mode = "all")
head(DiceSim)

#Create  data frame that contains the Dice similarity between any two vertices
F1 <- function(x) {data.frame(diceSim = DiceSim[x$SourceID +1, x$TargetID + 1])}
#Place a new column in edgeList with the Dice Sim
head(edgeList)
edgeList <- plyr::ddply(edgeList,
                        .variables=c("SourceName", "TargetName", "Weight", 
                                               "SourceID", "TargetID"), 
                        function(x) data.frame(F1(x)))
head(edgeList)
```

```{r}
##################################################################################
##################   color #################################################
######################################################
COLOR_P <- colorRampPalette(c("#00FF00", "#FF0000"), 
                            bias = nrow(edgeList), space = "rgb", 
                            interpolate = "linear")
COLOR_P
(colCodes <- COLOR_P(length(unique(edgeList$diceSim))))
edges_col <- sapply(edgeList$diceSim, 
                    function(x) colCodes[which(sort(unique(edgeList$diceSim)) == x)])
nrow(edges_col)


```

```{r}
library(networkD3)
D3_network_Reviews <- networkD3::forceNetwork(
  Links = edgeList, # data frame that contains info about edges
  Nodes = nodeList, # data frame that contains info about nodes
  Source = "SourceID", # ID of source node 
  Target = "TargetID", # ID of target node
  Value = "Weight", # value from the edge list (data frame) that will be used to value/weight relationship amongst nodes
  NodeID = "nName", # value from the node list (data frame) that contains node description we want to use (e.g., node name)
  Nodesize = "nodeBetweenness",  # value from the node list (data frame) that contains value we want to use for a node size
  Group = "nodeDegree",  # value from the node list (data frame) that contains value we want to use for node color
  height = 700, # Size of the plot (vertical)
  width = 900,  # Size of the plot (horizontal)
  fontSize = 20, # Font size
  linkDistance = networkD3::JS("function(d) { return d.value*10; }"), # Function to determine distance between any two nodes, uses variables already defined in forceNetwork function (not variables from a data frame)
  linkWidth = networkD3::JS("function(d) { return d.value/10; }"),# Function to determine link/edge thickness, uses variables already defined in forceNetwork function (not variables from a data frame)
  opacity = 0.9, # opacity
  zoom = TRUE, # ability to zoom when click on the node
  opacityNoHover = 0.9, # opacity of labels when static
  linkColour = "red"   ###"edges_col"red"# edge colors
) 

# Plot network
#D3_network_Reviews

# Save network as html file
networkD3::saveNetwork(D3_network_Reviews, 
                       "NetD3_top20_Lift.html", selfcontained = TRUE)

```







